{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS5803 NLP\n",
    "### Assignment 2\n",
    "#### Tanmay Garg, Tanmay Goyal, Tanay Yadav\n",
    "#### Roll no: CS20BTECH11063, AI20BTECH11021, AI20BTECH11026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Link to dataset: https://www.kaggle.com/datasets/moxxis/harry-potter-lstm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tanmaygoyal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tanmaygoyal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing necessary packages\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from math import log, exp  \n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1. Preprocess and tokenize the dataset using NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the original data\n",
    "with open('Harry_Potter_all_char_separated.txt', 'r', encoding='utf-8') as file:\n",
    "    harry_potter_data = file.read()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    '''\n",
    "    Function to preprocess the text by removing punctuations and converting to lower case\n",
    "    '''\n",
    "    # [^\\w\\s] -> ^ means except , \\w refers to any alphanumeric character and \\s refers to whitespace    \n",
    "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    '''\n",
    "    Function to tokenize the text\n",
    "    '''\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    # stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# preprocessing the text\n",
    "harry_potter_text = preprocess_text(harry_potter_data)\n",
    "# using the first 10000 words\n",
    "harry_potter_tokens = tokenize(harry_potter_text)[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2. Fit two bigram language models on the text: MLE and kneserNey Discounting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit two bigram language models on the text: MLE and Kneser-Ney discounting using the nltk library\n",
    "\n",
    "def MLE_bigram(n , n_gram , vocab):\n",
    "    '''\n",
    "    Function to fit a bigram language model using MLE\n",
    "    '''\n",
    "    model = nltk.lm.MLE(n)\n",
    "    model.fit([n_gram], vocabulary_text = vocab)\n",
    "    return model\n",
    "\n",
    "def KN_bigram(n , n_gram , vocab):\n",
    "    '''\n",
    "    Function to fit a bigram language model using Kneser-Ney discounting\n",
    "    '''\n",
    "    model = nltk.lm.KneserNeyInterpolated(n)\n",
    "    model.fit([n_gram], vocabulary_text = vocab)\n",
    "    return model\n",
    "\n",
    "harry_potter_bigrams = nltk.ngrams(harry_potter_tokens, 2)\n",
    "\n",
    "# converting the bigrams to a list\n",
    "bigrams = []\n",
    "for bigram in harry_potter_bigrams:\n",
    "    bigrams.append(bigram)\n",
    "\n",
    "bigram_mle = MLE_bigram(2 , bigrams , harry_potter_tokens)\n",
    "bigram_kn = KN_bigram(2 , bigrams , harry_potter_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3. Use the beginning words 1. \"Harry Potter\" and 2. \"Dumbledore\" to generate text using both the language models. Keep maximum text length as 20**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== MLE Model ====\n",
      "harry potter are are less like us all right of them angrily it he does tend to be nice if hed just\n",
      "dumbledore and bacon as dudley was a man had swapped at his favorite program had expected mrs dursleys bought dudley he\n",
      "==== KneserNey Model ====\n",
      "harry potter are are less like us all right place you cant ive finished dialing his hair and piers and had four\n",
      "dumbledore and bacon as dudley was a mad old things gray tuesday our heads down old clothes of arms and james\n"
     ]
    }
   ],
   "source": [
    "def generate_prediction(model , num_words = None , text_seed = None , random_seed = None):\n",
    "    '''\n",
    "    Function to generate the predictions given text_seed and num_words.\n",
    "    It joins them together in a sentence and returns the sentence\n",
    "    '''\n",
    "\n",
    "    # preprocessing the text_seed\n",
    "    text_seed = preprocess_text(text_seed)\n",
    "    text_seed = tokenize(text_seed)\n",
    "    \n",
    "    # generating the prediction\n",
    "    pred = model.generate(num_words = num_words , text_seed = text_seed , random_seed = random_seed)\n",
    "    \n",
    "    sentence = text_seed + [pred[i] for i in range(num_words)]\n",
    "    predicted_sentence = sentence[0]\n",
    "    for word in sentence[1:]:\n",
    "        predicted_sentence += ' ' + word\n",
    "    return predicted_sentence\n",
    "\n",
    "\n",
    "print(\"==== MLE Model ====\")\n",
    "print(generate_prediction(bigram_mle , num_words = 20 , text_seed = \"Harry Potter\" , random_seed = 123))\n",
    "print(generate_prediction(bigram_mle , num_words = 20 , text_seed = \"Dumbledore\" , random_seed = 123)) \n",
    "\n",
    "print(\"==== KneserNey Model ====\")\n",
    "print(generate_prediction(bigram_kn , num_words = 20 , text_seed = \"Harry Potter\" , random_seed = 123))\n",
    "print(generate_prediction(bigram_kn , num_words = 20 , text_seed = \"Dumbledore\" , random_seed = 123)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beam search is a tree-based search strategy similar to BFS. In BF, we expand every child node, however, in Beam Search, we only expand the top k most probable children. The generated text is the text with the highest probabiltity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4. To implement beam search, implement a function to find the top k most probable words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_top_probable(model , k , word):\n",
    "    '''\n",
    "    returns the top-k most probable words based on the model given\n",
    "    ''' \n",
    "    \n",
    "    # we store the non-zero probabilities\n",
    "    non_zero_prob = {}\n",
    "\n",
    "    for w in model.vocab:\n",
    "        if model.score(w , [word]) > 0:\n",
    "            non_zero_prob[w] = model.score(w , [word])\n",
    "\n",
    "    # sorting the non_zero_prob based on values\n",
    "    sorted(non_zero_prob.items() , key = lambda item:item[1])\n",
    "    \n",
    "    if len(non_zero_prob) > k:\n",
    "        return list(non_zero_prob.keys())[:k]\n",
    "    \n",
    "    else:\n",
    "        top = list(non_zero_prob.keys())\n",
    "\n",
    "        # we sample words randomly which had zero probability\n",
    "        random = []\n",
    "        for i in range(k - len(non_zero_prob)):\n",
    "            w = random.choice(model.vocab)\n",
    "            while w in top or w not in random:\n",
    "                w = random.choice(model.vocab)\n",
    "            random.append(w)\n",
    "\n",
    "        return top + random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5. Implement Beam Search using the MLE language model previously trained.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6. Repeat Q3 using Beam Search with k=2 and depth = 10. Find the 5 generated texts with highest probability for each of the three sentences.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
