{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForQuestionAnswering, BertModel\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Load model and retrieve attention weights\n",
    "\n",
    "from bertviz import head_view, model_view\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoTokenizer, BertForQuestionAnswering, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Embeddings: tensor([[ 0.0314,  0.0596,  0.0288,  ...,  0.0033,  0.0556,  0.0068],\n",
      "        [ 0.0211,  0.0423,  0.0354,  ...,  0.0096,  0.0508, -0.0001]])\n",
      "Cosine Similarity from embeddings generated by the model: 0.884947\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = [\"he is a good man\", \"he is a great man\"]\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "print('Sentence Embeddings:', sentence_embeddings)\n",
    "print('Cosine Similarity from embeddings generated by the model: {:4f}'.format(torch.cosine_similarity(sentence_embeddings[0].unsqueeze(0), sentence_embeddings[1].unsqueeze(0)).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_version = 'sentence-transformers/all-mpnet-base-v2'\n",
    "model = AutoModel.from_pretrained(model_version, output_attentions=True, output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_version)\n",
    "question = \"he is a good man\"\n",
    "text = \"he is a good man\"\n",
    "sentences=[\"he is a good man\", \"he is a good man\"]\n",
    "inputs = tokenizer(sentences, return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "print(inputs.keys())\n",
    "attention = model(input_ids)[-1]\n",
    "# sentence_b_start = token_type_ids[0].tolist().index(1)\n",
    "input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_id_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   0, 2006, 2007, 1041, 2208, 2162,    2],\n",
       "        [   0, 2006, 2007, 1041, 2208, 2162,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1\n",
      "tensor(0) : <s>\n",
      "tensor(2006) : he\n",
      "tensor(2007) : is\n",
      "tensor(1041) : a\n",
      "tensor(2208) : good\n",
      "tensor(2162) : man\n",
      "tensor(2) : </s>\n",
      "Sentence 2\n",
      "tensor(0) : <s>\n",
      "tensor(2006) : he\n",
      "tensor(2007) : is\n",
      "tensor(1041) : a\n",
      "tensor(2208) : good\n",
      "tensor(2162) : man\n",
      "tensor(2) : </s>\n"
     ]
    }
   ],
   "source": [
    "ticks = []\n",
    "for j in range(len(inputs.input_ids)):\n",
    "    print(f'Sentence {j+1}')\n",
    "    for i in range(len(inputs.input_ids[j])):\n",
    "        print(inputs.input_ids[0][i], ':', tokenizer.decode(inputs.input_ids[0][i]))\n",
    "        ticks.append(tokenizer.decode(inputs.input_ids[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 2, 12, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "attention_tensors = []\n",
    "\n",
    "for tensor in model(input_ids).attentions:\n",
    "    attention_tensors.append(tensor.detach().numpy())\n",
    "\n",
    "attentions_temp = torch.tensor(np.array(attention_tensors))\n",
    "print(attentions_temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1_len = len(question.split(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 sns.heatmap(attentions_temp.mean(axis=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>)[-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>][<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>].T[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>:sentence1_len+<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>,sentence1_len+<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>:] ,x     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\matrix.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">446</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">heatmap</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 443 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 444 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 445 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Initialize the plotter object</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 446 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>plotter = _HeatMapper(data, vmin, vmax, cmap, center, robust, annot, fmt,             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 447 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │     </span>annot_kws, cbar, cbar_kws, xticklabels,                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 448 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │     </span>yticklabels, mask)                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 449 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\matrix.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">163</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 160 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.ylabel = ylabel <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> ylabel <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #808000; text-decoration-color: #808000\">\"\"</span>                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 161 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 162 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Determine good default values for the colormapping</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 163 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._determine_cmap_params(plot_data, vmin, vmax,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 164 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   │   │   </span>cmap, center, robust)                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 165 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 166 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Sort out the annotations</span>                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\matrix.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">202</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_determine_cmap_params</span>       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 199 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> robust:                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 200 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>vmin = np.nanpercentile(calc_data, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 201 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 202 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>vmin = np.nanmin(calc_data)                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 203 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> vmax <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 204 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> robust:                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 205 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>vmax = np.nanpercentile(calc_data, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">98</span>)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">nanmin</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">200</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">c:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">343</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">nanmin</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 340 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(a) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> np.ndarray <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> a.dtype != np.object_:                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 341 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Fast, but not safe for subclasses of ndarray, or object arrays,</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 342 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># which do not implement isnan (gh-9009), or fmin correctly (gh-8975)</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 343 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>res = np.fmin.reduce(a, axis=axis, out=out, **kwargs)                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 344 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> np.isnan(res).any():                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 345 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>warnings.warn(<span style=\"color: #808000; text-decoration-color: #808000\">\"All-NaN slice encountered\"</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">RuntimeWarning</span>,                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 346 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │     </span>stacklevel=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>zero-size array to reduction operation fmin which has no identity\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 sns.heatmap(attentions_temp.mean(axis=\u001b[94m2\u001b[0m)[-\u001b[94m2\u001b[0m][\u001b[94m0\u001b[0m].T[\u001b[94m1\u001b[0m:sentence1_len+\u001b[94m2\u001b[0m,sentence1_len+\u001b[94m2\u001b[0m:] ,x     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\matrix.py\u001b[0m:\u001b[94m446\u001b[0m in \u001b[92mheatmap\u001b[0m                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 443 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 444 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 445 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Initialize the plotter object\u001b[0m                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 446 \u001b[2m│   \u001b[0mplotter = _HeatMapper(data, vmin, vmax, cmap, center, robust, annot, fmt,             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 447 \u001b[0m\u001b[2m│   │   │   │   │   │     \u001b[0mannot_kws, cbar, cbar_kws, xticklabels,                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 448 \u001b[0m\u001b[2m│   │   │   │   │   │     \u001b[0myticklabels, mask)                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 449 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\matrix.py\u001b[0m:\u001b[94m163\u001b[0m in \u001b[92m__init__\u001b[0m                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 160 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.ylabel = ylabel \u001b[94mif\u001b[0m ylabel \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[33m\"\u001b[0m\u001b[33m\"\u001b[0m                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 161 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 162 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Determine good default values for the colormapping\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 163 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._determine_cmap_params(plot_data, vmin, vmax,                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 164 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   \u001b[0mcmap, center, robust)                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 165 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 166 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Sort out the annotations\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\matrix.py\u001b[0m:\u001b[94m202\u001b[0m in \u001b[92m_determine_cmap_params\u001b[0m       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 199 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m robust:                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 200 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mvmin = np.nanpercentile(calc_data, \u001b[94m2\u001b[0m)                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 201 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 202 \u001b[2m│   │   │   │   \u001b[0mvmin = np.nanmin(calc_data)                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 203 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m vmax \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 204 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m robust:                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 205 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mvmax = np.nanpercentile(calc_data, \u001b[94m98\u001b[0m)                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mnanmin\u001b[0m:\u001b[94m200\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py\u001b[0m:\u001b[94m343\u001b[0m in \u001b[92mnanmin\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 340 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mtype\u001b[0m(a) \u001b[95mis\u001b[0m np.ndarray \u001b[95mand\u001b[0m a.dtype != np.object_:                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 341 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Fast, but not safe for subclasses of ndarray, or object arrays,\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 342 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# which do not implement isnan (gh-9009), or fmin correctly (gh-8975)\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 343 \u001b[2m│   │   \u001b[0mres = np.fmin.reduce(a, axis=axis, out=out, **kwargs)                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 344 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m np.isnan(res).any():                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 345 \u001b[0m\u001b[2m│   │   │   \u001b[0mwarnings.warn(\u001b[33m\"\u001b[0m\u001b[33mAll-NaN slice encountered\u001b[0m\u001b[33m\"\u001b[0m, \u001b[96mRuntimeWarning\u001b[0m,                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 346 \u001b[0m\u001b[2m│   │   │   │   │   │     \u001b[0mstacklevel=\u001b[94m3\u001b[0m)                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0mzero-size array to reduction operation fmin which has no identity\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(attentions_temp.mean(axis=2)[-2][0].T[1:sentence1_len+2,sentence1_len+2:] ,xticklabels = ticks[sentence1_len+2:] , yticklabels = ticks[1:sentence1_len+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0421, 0.0409, 0.0336, 0.0095, 0.0172, 0.0474],\n",
       "        [0.0193, 0.0492, 0.0426, 0.0117, 0.0106, 0.0570],\n",
       "        [0.0120, 0.0302, 0.0290, 0.0100, 0.0077, 0.0190],\n",
       "        [0.0188, 0.0537, 0.0471, 0.0344, 0.0193, 0.0451],\n",
       "        [0.0439, 0.0358, 0.0449, 0.0202, 0.0317, 0.0672],\n",
       "        [0.0240, 0.0332, 0.0316, 0.0130, 0.0147, 0.0244]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions_temp.mean(axis=2)[-2][0].T[1:sentence1_len+2,sentence1_len+2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2002, 2003, 1037, 2204, 2158,  102, 2002, 2003, 1037, 2204, 2158,\n",
       "          102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(question, text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    " \n",
    " \n",
    "def get_word_idx(sent: str, word: str):\n",
    "    return sent.split(\" \").index(word)\n",
    " \n",
    " \n",
    "def get_hidden_states(encoded, token_ids_word, model, layers):\n",
    "    \"\"\"Push input IDs through model. Stack and sum `layers` (last four by default).\n",
    "        Select only those subword token outputs that belong to our word of interest\n",
    "        and average them.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded)\n",
    " \n",
    "     # Get all hidden states\n",
    "    states = output.hidden_states\n",
    "     # Stack and sum all requested layers\n",
    "    output = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n",
    "     # Only select the tokens that constitute the requested word\n",
    "    word_tokens_output = output[token_ids_word]\n",
    " \n",
    "    return word_tokens_output.mean(dim=0)\n",
    " \n",
    " \n",
    "def get_word_vector(sent, idx, tokenizer, model, layers):\n",
    "    \"\"\"Get a word vector by first tokenizing the input sentence, getting all token idxs\n",
    "        that make up the word of interest, and then `get_hidden_states`.\"\"\"\n",
    "    encoded = tokenizer.encode_plus(sent, return_tensors=\"pt\")\n",
    "     # get all token idxs that belong to the word of interest\n",
    "    token_ids_word = np.where(np.array(encoded.word_ids()) == idx)\n",
    " \n",
    "    return get_hidden_states(encoded, token_ids_word, model, layers)\n",
    " \n",
    " \n",
    "def main(layers , sent1 , sent2):\n",
    "    # Use last four layers by default\n",
    "    layers = [-4, -3, -2, -1] if layers is None else layers\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    model = AutoModel.from_pretrained(\"bert-base-cased\", output_hidden_states=True)\n",
    "\n",
    "    sent1_embeddings = []\n",
    "    sent2_embeddings = []\n",
    "    sent1 = sent1\n",
    "    sent2 = sent2\n",
    "    for w in sent1.split(\" \"):\n",
    "        sent1_embeddings.append(get_word_vector(sent1, get_word_idx(sent1, w), tokenizer, model, layers))\n",
    "    for w in sent2.split(\" \"):\n",
    "        sent2_embeddings.append(get_word_vector(sent2 , get_word_idx(sent2, w), tokenizer, model, layers))\n",
    "    # idx = get_word_idx(sent, \"cookies\")\n",
    "\n",
    "    # word_embedding = get_word_vector(sent, idx, tokenizer, model, layers)\n",
    "     \n",
    "    return sent1_embeddings, sent2_embeddings \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he ~ he ; Cosine Distance: 1.000000\n",
      "he ~ is ; Cosine Distance: 0.790254\n",
      "he ~ a ; Cosine Distance: 0.717984\n",
      "he ~ good ; Cosine Distance: 0.688151\n",
      "he ~ man ; Cosine Distance: 0.763715\n",
      "is ~ he ; Cosine Distance: 0.790254\n",
      "is ~ is ; Cosine Distance: 1.000000\n",
      "is ~ a ; Cosine Distance: 0.865570\n",
      "is ~ good ; Cosine Distance: 0.786840\n",
      "is ~ man ; Cosine Distance: 0.787070\n",
      "a ~ he ; Cosine Distance: 0.717984\n",
      "a ~ is ; Cosine Distance: 0.865570\n",
      "a ~ a ; Cosine Distance: 1.000000\n",
      "a ~ good ; Cosine Distance: 0.800178\n",
      "a ~ man ; Cosine Distance: 0.746675\n",
      "good ~ he ; Cosine Distance: 0.688151\n",
      "good ~ is ; Cosine Distance: 0.786840\n",
      "good ~ a ; Cosine Distance: 0.800178\n",
      "good ~ good ; Cosine Distance: 1.000000\n",
      "good ~ man ; Cosine Distance: 0.762201\n",
      "man ~ he ; Cosine Distance: 0.763715\n",
      "man ~ is ; Cosine Distance: 0.787070\n",
      "man ~ a ; Cosine Distance: 0.746675\n",
      "man ~ good ; Cosine Distance: 0.762201\n",
      "man ~ man ; Cosine Distance: 1.000000\n"
     ]
    }
   ],
   "source": [
    "sent1 = question\n",
    "sent2 = text\n",
    "s1 , s2 = main(None , sent1 , sent2)\n",
    "sentence1 = sent1.split(\" \")\n",
    "sentence2 = sent2.split(\" \")\n",
    "for idx1 , v1 in enumerate(s1):\n",
    "    for idx2 , v2 in enumerate(s2):\n",
    "        print('{} ~ {} ; Cosine Distance: {:5f}' .format(sentence1[idx1], sentence2[idx2], torch.cosine_similarity(v1.unsqueeze(0), v2.unsqueeze(0)).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 he tensor(5)\n",
      "1 is tensor(5)\n",
      "2 a tensor(1)\n",
      "a is tensor([0.8656])\n",
      "3 good tensor(1)\n",
      "good is tensor([0.7868])\n",
      "4 man tensor(5)\n",
      "tensor([1.6524])\n",
      "tensor([0.3305])\n"
     ]
    }
   ],
   "source": [
    "attention_second_last = attentions_temp.mean(axis=2)[-2][0].T[1:sentence1_len+2,sentence1_len+2:] \n",
    "sum = 0\n",
    "for r in range(attention_second_last.shape[0]-1):\n",
    "    max_idx = np.argmax(attention_second_last[r])\n",
    "    print(r , sentence1[r] , max_idx)\n",
    "    if max_idx == len(attention_second_last[r])-1:\n",
    "        continue\n",
    "    print(sentence1[r] , sentence2[max_idx] , torch.cosine_similarity(s1[r].unsqueeze(0), s2[max_idx].unsqueeze(0)))\n",
    "    sum+=torch.cosine_similarity(s1[r].unsqueeze(0), s2[max_idx].unsqueeze(0))\n",
    "\n",
    "\n",
    "print(sum)\n",
    "print(sum/len(question.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
