{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForQuestionAnswering, BertModel, AutoModel\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import rouge_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Sentence embeddings from \"all-mpnet-base-v2\" and calculating cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Embeddings: tensor([[ 0.0314,  0.0596,  0.0288,  ...,  0.0033,  0.0556,  0.0068],\n",
      "        [ 0.0211,  0.0423,  0.0354,  ...,  0.0096,  0.0508, -0.0001]])\n",
      "Cosine Similarity from embeddings generated by the model: 0.884947\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "question = \"he is a good man\"\n",
    "text = \"he is a good man\"\n",
    "sentences = [\"he is a good man\", \"he is a great man\"]\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "print('Sentence Embeddings:', sentence_embeddings)\n",
    "print('Cosine Similarity from embeddings generated by the model: {:4f}'.format(torch.cosine_similarity(sentence_embeddings[0].unsqueeze(0), sentence_embeddings[1].unsqueeze(0)).item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating our similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ticks_attention_scores(sentence1 , sentence2):\n",
    "    model_version = 'sentence-transformers/all-mpnet-base-v2'\n",
    "    model = AutoModel.from_pretrained(model_version, output_attentions=True, output_hidden_states=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_version)\n",
    "\n",
    "    # generating the embeddings\n",
    "    inputs = tokenizer(sentence1 , sentence2 , return_tensors='pt')\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention = model(input_ids)[-1]\n",
    "    input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_id_list) \n",
    "\n",
    "    # generating the ticks\n",
    "    ticks = []\n",
    "    for j in range(len(inputs.input_ids)):\n",
    "        for i in range(len(inputs.input_ids[j])):\n",
    "            ticks.append(tokenizer.decode(inputs.input_ids[0][i]))\n",
    "\n",
    "    # generating the attention matrix\n",
    "    attention_tensors = []\n",
    "    for tensor in model(input_ids).attentions:\n",
    "        attention_tensors.append(tensor.detach().numpy())\n",
    "    attentions_temp = torch.tensor(np.array(attention_tensors))\n",
    "    \n",
    "    return attentions_temp , ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_idx(sent: str, word: str):\n",
    "    return sent.split(\" \").index(word)\n",
    " \n",
    " \n",
    "def get_hidden_states(encoded, token_ids_word, model, layers):\n",
    "    \"\"\"Push input IDs through model. Stack and sum `layers` (last four by default).\n",
    "        Select only those subword token outputs that belong to our word of interest\n",
    "        and average them.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded)\n",
    " \n",
    "    # Get all hidden states\n",
    "    states = output.hidden_states\n",
    "    # Stack and sum all requested layers\n",
    "    output = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n",
    "    # Only select the tokens that constitute the requested word\n",
    "    word_tokens_output = output[token_ids_word]\n",
    " \n",
    "    return word_tokens_output.mean(dim=0)\n",
    " \n",
    " \n",
    "def get_word_vector(sent, idx, tokenizer, model, layers):\n",
    "    \"\"\"Get a word vector by first tokenizing the input sentence, getting all token idxs\n",
    "        that make up the word of interest, and then `get_hidden_states`.\"\"\"\n",
    "    encoded = tokenizer.encode_plus(sent, return_tensors=\"pt\")\n",
    "    # get all token idxs that belong to the word of interest\n",
    "    token_ids_word = np.where(np.array(encoded.word_ids()) == idx)\n",
    " \n",
    "    return get_hidden_states(encoded, token_ids_word, model, layers)\n",
    " \n",
    " \n",
    "def get_BERT_embeddings(layers , sent1 , sent2):\n",
    "    # Use last four layers by default\n",
    "    layers = [-4, -3, -2, -1] if layers is None else layers\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = AutoModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
    "\n",
    "    sent1_embeddings = []\n",
    "    sent2_embeddings = []\n",
    "    for w in sent1.split(\" \"):\n",
    "        sent1_embeddings.append(get_word_vector(sent1, get_word_idx(sent1, w), tokenizer, model, layers))\n",
    "    for w in sent2.split(\" \"):\n",
    "        sent2_embeddings.append(get_word_vector(sent2 , get_word_idx(sent2, w), tokenizer, model, layers))\n",
    "    # print(type(sent1_embeddings), type(sent2_embeddings), type(sent1_embeddings[0]))\n",
    "    return sent1_embeddings, sent2_embeddings \n",
    "\n",
    "def get_Word2Vec_embeddings(sent1 , sent2):\n",
    "    from gensim.models import Word2Vec\n",
    "    from gensim.models import KeyedVectors\n",
    "    import gensim.downloader as api\n",
    "    word_vectors = api.load(\"word2vec-google-news-300\")\n",
    "    sent1_embeddings = []\n",
    "    sent2_embeddings = []\n",
    "    for w in sent1.split(\" \"):\n",
    "        sent1_embeddings.append(torch.tensor(word_vectors[w]))\n",
    "    for w in sent2.split(\" \"):\n",
    "        sent2_embeddings.append(torch.tensor(word_vectors[w]))\n",
    "    return sent1_embeddings, sent2_embeddings\n",
    "\n",
    "\n",
    "def get_glove_embeddings(sent1 , sent2):\n",
    "    from gensim.models import Word2Vec\n",
    "    from gensim.models import KeyedVectors\n",
    "    import gensim.downloader as api\n",
    "    word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "    sent1_embeddings = []\n",
    "    sent2_embeddings = []\n",
    "    for w in sent1.split(\" \"):\n",
    "        sent1_embeddings.append(torch.tensor(word_vectors[w]))\n",
    "    for w in sent2.split(\" \"):\n",
    "        sent2_embeddings.append(torch.tensor(word_vectors[w]))\n",
    "    return sent1_embeddings, sent2_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(attention_matrix , s1_embeddings , s2_embeddings):\n",
    "    sum = 0\n",
    "    len_s1 = attention_matrix.shape[0]-2 # to account for the [SEP]\n",
    "    len_s2 = attention_matrix.shape[1]-2 # to account foe the [SEP]\n",
    "\n",
    "    if len(s1_embeddings) != len_s1 or len(s2_embeddings) != len_s2:\n",
    "        print(\"Incorrect dimensions of embeddings or attention matrix\")\n",
    "        print(\"s1_embeddings: \", len(s1_embeddings))\n",
    "        print(\"s2_embeddings: \", len(s2_embeddings))\n",
    "        print(\"attention_matrix: \", attention_matrix.shape)\n",
    "        return\n",
    "    \n",
    "    for i in range(len_s1):\n",
    "        correct_idx = i+1 # since we have [SEP] at 0 index\n",
    "        attention = attention_matrix[correct_idx]\n",
    "        sorted_indices = np.argsort(attention)\n",
    "        if sorted_indices[-1] not in [0 , len(attention)-1]:\n",
    "            most_attention = sorted_indices[-1]\n",
    "        elif sorted_indices[-2] not in [0 , len(attention)-1]:\n",
    "            most_attention = sorted_indices[-2]\n",
    "        else:\n",
    "            most_attention = sorted_indices[-3]\n",
    "        most_attention_correct_idx = most_attention - 1  # since 0 index is SEP\n",
    "        sum += torch.cosine_similarity(s1_embeddings[i].unsqueeze(0) , s2_embeddings[most_attention_correct_idx].unsqueeze(0)).item()\n",
    "\n",
    "    full_score =sum / len_s1\n",
    "\n",
    "    return full_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(sentence1 , sentence2 , plot_attention = True, embeddings='glove'):\n",
    "    '''\n",
    "    Function to calculate the similarity score between two sentences\n",
    "    Parameters:\n",
    "    sentence1 : sentence 1\n",
    "    sentence2 : sentence 2\n",
    "    plot_attention : Whether to plot the attention matrix or not    \n",
    "    '''\n",
    "\n",
    "    \n",
    "    sentence1 = sentence1.lower()\n",
    "    sentence2 = sentence2.lower()\n",
    "\n",
    "\n",
    "    attention_matrix , ticks = get_ticks_attention_scores(sentence1 , sentence2)\n",
    "    if embeddings == 'bert':\n",
    "        embeddings_s1 , embeddings_s2 = get_BERT_embeddings(None , sentence1 , sentence2)\n",
    "    if embeddings == 'word2vec':\n",
    "        embeddings_s1 , embeddings_s2 = get_Word2Vec_embeddings(sentence1 , sentence2)\n",
    "    if embeddings == 'glove':\n",
    "        embeddings_s1 , embeddings_s2 = get_glove_embeddings(sentence1 , sentence2)\n",
    "    # embeddings_s1 , embeddings_s2 = get_Word2Vec_embeddings(sentence1 , sentence2)\n",
    "    \n",
    "    sep_indices = [i for i in range(len(ticks)) if ticks[i] in [\"<s>\" , \"</s>\"]]\n",
    "    if len(sep_indices) != 4:\n",
    "        print(\"Incorrect number of [SEP] tokens\")\n",
    "        return\n",
    "    \n",
    "    sentence_divider = sep_indices[1]\n",
    "\n",
    "    relevant_attention_matrix = []\n",
    "    # for A->B\n",
    "    relevant_attention_matrix.append(attention_matrix.mean(axis=2)[-1][0][:sentence_divider+1,sentence_divider+1:])\n",
    "    # for B->A\n",
    "    relevant_attention_matrix.append(attention_matrix.mean(axis=2)[-1][0][sentence_divider+1:,:sentence_divider+1])\n",
    "    \n",
    "    if plot_attention:\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.title(\"Attention from Reference -> Generated\")\n",
    "        sns.heatmap(relevant_attention_matrix[0],xticklabels = ticks[sentence_divider+1:] , yticklabels = ticks[:sentence_divider+1], annot=True)\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.title(\"Attention Matrix from Generated -> Reference\")\n",
    "        sns.heatmap(relevant_attention_matrix[1],xticklabels = ticks[:sentence_divider+1] , yticklabels = ticks[sentence_divider+1:], annot=True)\n",
    "        plt.show()\n",
    "\n",
    "    # getting the scores\n",
    "    return max(get_scores(relevant_attention_matrix[0] , embeddings_s1 , embeddings_s2) , get_scores(relevant_attention_matrix[1] , embeddings_s2 , embeddings_s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score with Glove:  0.822573447227478\n",
      "Similarity Score with BERT:  0.7127435266971588\n",
      "Rouge Score:  {'rouge1': 0.8000000000000002, 'rouge2': 0.5, 'rougeL': 0.8000000000000002, 'rougeLsum': 0.8000000000000002}\n",
      "Similarity Score with Glove:  0.6111858822405338\n",
      "Similarity Score with BERT:  0.4353034757077694\n",
      "Rouge Score:  {'rouge1': 0.6153846153846154, 'rouge2': 0.36363636363636365, 'rougeL': 0.6153846153846154, 'rougeLsum': 0.6153846153846154}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reference = \"he is the good man\"\n",
    "predict1 = \"he is the best man\"\n",
    "predict2 = \"he is the best man of the groom\"\n",
    "\n",
    "print(\"Similarity Score with Glove: \", similarity_score(reference , predict1 , plot_attention=False , embeddings='glove'))\n",
    "print(\"Similarity Score with BERT: \", similarity_score(reference , predict1 , plot_attention=False , embeddings='bert'))\n",
    "rogue = evaluate.load('rouge')\n",
    "print(\"Rouge Score: \", rogue.compute(predictions=[predict1] , references=[reference]))\n",
    "\n",
    "print(\"Similarity Score with Glove: \", similarity_score(reference , predict2 , plot_attention=False , embeddings='glove'))\n",
    "print(\"Similarity Score with BERT: \", similarity_score(reference , predict2 , plot_attention=False , embeddings='bert'))\n",
    "# rouge = evaluate.load('rouge')\n",
    "print(\"Rouge Score: \", rogue.compute(predictions=[predict2] , references=[reference]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score with Glove for sentence 1:  0.5031808316707611\n",
      "Similarity Score with Glove for sentence 2: 0.43864552889551434\n",
      "Similarity Score with Glove for sentence 3:  0.47408916694777353\n",
      "Similarity Score with BERT for sentence 1:  0.5733822456427983\n",
      "Similarity Score with BERT for sentence 2:  0.5364494749477932\n",
      "Similarity Score with BERT for sentence 3:  0.5168611109256744\n",
      "Rouge Score for sentence 1:  {'rouge1': 0.42857142857142855, 'rouge2': 0.16666666666666666, 'rougeL': 0.42857142857142855, 'rougeLsum': 0.42857142857142855}\n",
      "Rouge Score for sentence 2:  {'rouge1': 0.42857142857142855, 'rouge2': 0.16666666666666666, 'rougeL': 0.42857142857142855, 'rougeLsum': 0.42857142857142855}\n",
      "Rouge Score for sentence 3:  {'rouge1': 0.5714285714285714, 'rouge2': 0.3333333333333333, 'rougeL': 0.5714285714285714, 'rougeLsum': 0.5714285714285714}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reference = \"he is taking a flight to mumbai\"\n",
    "predict1 = \"he is coming in mumbai by airplane\"\n",
    "predict2 = \"he is coming from mumbai by airplane\"\n",
    "predict3 = \"he is travelling via air to mumbai\"\n",
    "\n",
    "print(\"Similarity Score with Glove for sentence 1: \", similarity_score(reference , predict1 , plot_attention=False , embeddings='glove'))\n",
    "print(\"Similarity Score with Glove for sentence 2:\", similarity_score(reference , predict2 , plot_attention=False , embeddings='glove'))\n",
    "print(\"Similarity Score with Glove for sentence 3: \", similarity_score(reference , predict3 , plot_attention=False , embeddings='glove'))\n",
    "print(\"Similarity Score with BERT for sentence 1: \", similarity_score(reference , predict1 , plot_attention=False , embeddings='bert'))\n",
    "print(\"Similarity Score with BERT for sentence 2: \", similarity_score(reference , predict2 , plot_attention=False , embeddings='bert'))\n",
    "print(\"Similarity Score with BERT for sentence 3: \", similarity_score(reference , predict3 , plot_attention=False , embeddings='bert'))\n",
    "rogue = evaluate.load('rouge')\n",
    "print(\"Rouge Score for sentence 1: \", rogue.compute(predictions=[predict1] , references=[reference]))\n",
    "print(\"Rouge Score for sentence 2: \", rogue.compute(predictions=[predict2] , references=[reference]))\n",
    "print(\"Rouge Score for sentence 3: \", rogue.compute(predictions=[predict3] , references=[reference]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score with Glove for sentence 1:  0.6746925839356014\n",
      "Similarity Score with BERT for sentence 1:  0.6548737202371869\n",
      "Rouge Score for sentence 1:  {'rouge1': 0.75, 'rouge2': 0.5714285714285715, 'rougeL': 0.75, 'rougeLsum': 0.75}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reference = \"he is taking a flight to mumbai\"\n",
    "predict1 = \"he is taking a plane to go to mumbai\"\n",
    "\n",
    "print(\"Similarity Score with Glove for sentence 1: \", similarity_score(reference , predict1 , plot_attention=False , embeddings='glove'))\n",
    "# print(\"Similarity Score with Glove for sentence 2:\", similarity_score(reference , predict2 , plot_attention=False , embeddings='glove'))\n",
    "# print(\"Similarity Score with Glove for sentence 3: \", similarity_score(reference , predict3 , plot_attention=False , embeddings='glove'))\n",
    "print(\"Similarity Score with BERT for sentence 1: \", similarity_score(reference , predict1 , plot_attention=False , embeddings='bert'))\n",
    "# print(\"Similarity Score with BERT for sentence 2: \", similarity_score(reference , predict2 , plot_attention=False , embeddings='bert'))\n",
    "# print(\"Similarity Score with BERT for sentence 3: \", similarity_score(reference , predict3 , plot_attention=False , embeddings='bert'))\n",
    "rogue = evaluate.load('rouge')\n",
    "print(\"Rouge Score for sentence 1: \", rogue.compute(predictions=[predict1] , references=[reference]))\n",
    "# print(\"Rouge Score for sentence 2: \", rogue.compute(predictions=[predict2] , references=[reference]))\n",
    "# print(\"Rouge Score for sentence 3: \", rogue.compute(predictions=[predict3] , references=[reference]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score with Glove for sentence 1:  0.6879322273390633\n",
      "Similarity Score with BERT for sentence 1:  0.6449346244335175\n",
      "Rouge Score for sentence 1:  {'rouge1': 0.42857142857142855, 'rouge2': 0.0, 'rougeL': 0.2857142857142857, 'rougeLsum': 0.2857142857142857}\n"
     ]
    }
   ],
   "source": [
    "reference = \"he is taking a flight to mumbai\"\n",
    "predict1 = \"he will arrive in mumbai by flight\"\n",
    "print(\"Similarity Score with Glove for sentence 1: \", similarity_score(reference , predict1 , plot_attention=False , embeddings='glove'))\n",
    "# print(\"Similarity Score with Glove for sentence 2:\", similarity_score(reference , predict2 , plot_attention=False , embeddings='glove'))\n",
    "# print(\"Similarity Score with Glove for sentence 3: \", similarity_score(reference , predict3 , plot_attention=False , embeddings='glove'))\n",
    "print(\"Similarity Score with BERT for sentence 1: \", similarity_score(reference , predict1 , plot_attention=False , embeddings='bert'))\n",
    "# print(\"Similarity Score with BERT for sentence 2: \", similarity_score(reference , predict2 , plot_attention=False , embeddings='bert'))\n",
    "# print(\"Similarity Score with BERT for sentence 3: \", similarity_score(reference , predict3 , plot_attention=False , embeddings='bert'))\n",
    "rogue = evaluate.load('rouge')\n",
    "print(\"Rouge Score for sentence 1: \", rogue.compute(predictions=[predict1] , references=[reference]))\n",
    "# print(\"Rouge Score for sentence 2: \", rogue.compute(predictions=[predict2] , references=[reference]))\n",
    "# print(\"Rouge Score for sentence 3: \", rogue.compute(predictions=[predict3] , references=[reference]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score with Glove for sentence 1:  0.4561354773385184\n",
      "Similarity Score with BERT for sentence 1:  0.6606973452227456\n",
      "Rouge Score for sentence 1:  {'rouge1': 0.7142857142857143, 'rouge2': 0.5, 'rougeL': 0.7142857142857143, 'rougeLsum': 0.7142857142857143}\n"
     ]
    }
   ],
   "source": [
    "reference = \"he is arriving in Mumbai by flight\"\n",
    "predict1 = \"he is departing from Mumbai by flight\"\n",
    "print(\"Similarity Score with Glove for sentence 1: \", similarity_score(reference , predict1 , plot_attention=False , embeddings='glove'))\n",
    "# print(\"Similarity Score with Glove for sentence 2:\", similarity_score(reference , predict2 , plot_attention=False , embeddings='glove'))\n",
    "# print(\"Similarity Score with Glove for sentence 3: \", similarity_score(reference , predict3 , plot_attention=False , embeddings='glove'))\n",
    "print(\"Similarity Score with BERT for sentence 1: \", similarity_score(reference , predict1 , plot_attention=False , embeddings='bert'))\n",
    "# print(\"Similarity Score with BERT for sentence 2: \", similarity_score(reference , predict2 , plot_attention=False , embeddings='bert'))\n",
    "# print(\"Similarity Score with BERT for sentence 3: \", similarity_score(reference , predict3 , plot_attention=False , embeddings='bert'))\n",
    "rogue = evaluate.load('rouge')\n",
    "print(\"Rouge Score for sentence 1: \", rogue.compute(predictions=[predict1] , references=[reference]))\n",
    "# print(\"Rouge Score for sentence 2: \", rogue.compute(predictions=[predict2] , references=[reference]))\n",
    "# print(\"Rouge Score for sentence 3: \", rogue.compute(predictions=[predict3] , references=[reference]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score with Glove for sentence 1:  0.8774118920167288\n",
      "Similarity Score with BERT for sentence 1:  0.7627855042616526\n",
      "Rouge Score for sentence 1:  {'rouge1': 0.7272727272727272, 'rouge2': 0.4444444444444445, 'rougeL': 0.7272727272727272, 'rougeLsum': 0.7272727272727272}\n"
     ]
    }
   ],
   "source": [
    "reference = \"i will go play basketball\"\n",
    "predict1 = \"i want to go play basketball\"\n",
    "print(\"Similarity Score with Glove for sentence 1: \", similarity_score(reference , predict1 , plot_attention=False , embeddings='glove'))\n",
    "# print(\"Similarity Score with Glove for sentence 2:\", similarity_score(reference , predict2 , plot_attention=False , embeddings='glove'))\n",
    "# print(\"Similarity Score with Glove for sentence 3: \", similarity_score(reference , predict3 , plot_attention=False , embeddings='glove'))\n",
    "print(\"Similarity Score with BERT for sentence 1: \", similarity_score(reference , predict1 , plot_attention=False , embeddings='bert'))\n",
    "# print(\"Similarity Score with BERT for sentence 2: \", similarity_score(reference , predict2 , plot_attention=False , embeddings='bert'))\n",
    "# print(\"Similarity Score with BERT for sentence 3: \", similarity_score(reference , predict3 , plot_attention=False , embeddings='bert'))\n",
    "rogue = evaluate.load('rouge')\n",
    "print(\"Rouge Score for sentence 1: \", rogue.compute(predictions=[predict1] , references=[reference]))\n",
    "# print(\"Rouge Score for sentence 2: \", rogue.compute(predictions=[predict2] , references=[reference]))\n",
    "# print(\"Rouge Score for sentence 3: \", rogue.compute(predictions=[predict3] , references=[reference]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score with Glove for sentence 1:  0.7309491634368896\n",
      "Similarity Score with Glove for sentence 2: 0.8225776354471842\n",
      "Similarity Score with Glove for sentence 3:  0.809838630259037\n",
      "Similarity Score with BERT for sentence 1:  0.6402829974889755\n",
      "Similarity Score with BERT for sentence 2:  0.7753874560197195\n",
      "Similarity Score with BERT for sentence 3:  0.6863011308014393\n",
      "Rouge Score for sentence 1:  {'rouge1': 0.6666666666666666, 'rouge2': 0.46153846153846156, 'rougeL': 0.6666666666666666, 'rougeLsum': 0.6666666666666666}\n",
      "Rouge Score for sentence 2:  {'rouge1': 0.9090909090909091, 'rouge2': 0.6666666666666665, 'rougeL': 0.9090909090909091, 'rougeLsum': 0.9090909090909091}\n",
      "Rouge Score for sentence 3:  {'rouge1': 0.7692307692307693, 'rouge2': 0.5454545454545454, 'rougeL': 0.7692307692307693, 'rougeLsum': 0.7692307692307693}\n"
     ]
    }
   ],
   "source": [
    "reference = \"i will watch a movie\"\n",
    "predict1 = \"i will go to the theatre to watch a movie\"\n",
    "predict2 = \"i will go watch a movie\"\n",
    "predict3 = \"i will go home to watch a movie\"\n",
    "print(\"Similarity Score with Glove for sentence 1: \", similarity_score(reference , predict1 , plot_attention=False , embeddings='glove'))\n",
    "print(\"Similarity Score with Glove for sentence 2:\", similarity_score(reference , predict2 , plot_attention=False , embeddings='glove'))\n",
    "print(\"Similarity Score with Glove for sentence 3: \", similarity_score(reference , predict3 , plot_attention=False , embeddings='glove'))\n",
    "print(\"Similarity Score with BERT for sentence 1: \", similarity_score(reference , predict1 , plot_attention=False , embeddings='bert'))\n",
    "print(\"Similarity Score with BERT for sentence 2: \", similarity_score(reference , predict2 , plot_attention=False , embeddings='bert'))\n",
    "print(\"Similarity Score with BERT for sentence 3: \", similarity_score(reference , predict3 , plot_attention=False , embeddings='bert'))\n",
    "rogue = evaluate.load('rouge')\n",
    "print(\"Rouge Score for sentence 1: \", rogue.compute(predictions=[predict1] , references=[reference]))\n",
    "print(\"Rouge Score for sentence 2: \", rogue.compute(predictions=[predict2] , references=[reference]))\n",
    "print(\"Rouge Score for sentence 3: \", rogue.compute(predictions=[predict3] , references=[reference]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score with Glove for sentence 1:  0.7584634678704398\n",
      "Similarity Score with Glove for sentence 2: 0.7390412290891012\n",
      "Similarity Score with Glove for sentence 3:  0.6539148489634196\n",
      "Similarity Score with BERT for sentence 1:  0.6863562266031901\n",
      "Similarity Score with BERT for sentence 2:  0.5725964903831482\n",
      "Similarity Score with BERT for sentence 3:  0.5595284054676691\n",
      "Rouge Score for sentence 1:  {'rouge1': 0.4615384615384615, 'rouge2': 0.1818181818181818, 'rougeL': 0.4615384615384615, 'rougeLsum': 0.4615384615384615}\n",
      "Rouge Score for sentence 2:  {'rouge1': 0.5, 'rouge2': 0.20000000000000004, 'rougeL': 0.5, 'rougeLsum': 0.5}\n",
      "Rouge Score for sentence 3:  {'rouge1': 0.5454545454545454, 'rouge2': 0.22222222222222224, 'rougeL': 0.5454545454545454, 'rougeLsum': 0.5454545454545454}\n"
     ]
    }
   ],
   "source": [
    "reference = \"i will go see a movie\"\n",
    "predict1 = \"i am going to watch a movie\"\n",
    "predict2 = \"i will watch a film now\"\n",
    "predict3 = \"i am watching a movie\"\n",
    "print(\"Similarity Score with Glove for sentence 1: \", similarity_score(reference , predict1 , plot_attention=False , embeddings='glove'))\n",
    "print(\"Similarity Score with Glove for sentence 2:\", similarity_score(reference , predict2 , plot_attention=False , embeddings='glove'))\n",
    "print(\"Similarity Score with Glove for sentence 3: \", similarity_score(reference , predict3 , plot_attention=False , embeddings='glove'))\n",
    "print(\"Similarity Score with BERT for sentence 1: \", similarity_score(reference , predict1 , plot_attention=False , embeddings='bert'))\n",
    "print(\"Similarity Score with BERT for sentence 2: \", similarity_score(reference , predict2 , plot_attention=False , embeddings='bert'))\n",
    "print(\"Similarity Score with BERT for sentence 3: \", similarity_score(reference , predict3 , plot_attention=False , embeddings='bert'))\n",
    "rogue = evaluate.load('rouge')\n",
    "print(\"Rouge Score for sentence 1: \", rogue.compute(predictions=[predict1] , references=[reference]))\n",
    "print(\"Rouge Score for sentence 2: \", rogue.compute(predictions=[predict2] , references=[reference]))\n",
    "print(\"Rouge Score for sentence 3: \", rogue.compute(predictions=[predict3] , references=[reference]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
